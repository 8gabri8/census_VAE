#!/bin/bash

# SLURM settings
#SBATCH --job-name=census_job   
#SBATCH --nodes 10
#SBATCH --ntasks 3   
#SBATCH --cpus-per-task 15
#SBATCH --time 1:00:00
#SBATCH --mem=800GB              # Memory limit
#SBATCH --partition=bigmem
#SBATCH --output=/scratch/dalai/census_VAE/log/3_VAE_train_%j.out

JOB_ID=$SLURM_JOB_ID  # This automatically gets the job ID assigned by Slurm

export PYTHONUNBUFFERED=1 # force to write the ourput fiel while running and not just at the end

# Load required modules (if applicable)
#module load python/3.8  # Replace with your Python version/module

# Activate the virtual environment
#source /home/dalai/miniconda3/envs/census_env/bin/activate  # Replace with the actual path

# Run the Python script
python ./../model/train_parallel.py --job_id $JOB_ID

# Deactivate the virtual environment (optional, but good practice)
#deactivate
